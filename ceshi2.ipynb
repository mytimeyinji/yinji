{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ceshi1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYVprS5DyayQPjfhetDQFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mytimeyinji/yinji/blob/master/ceshi2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3owCHtR3eHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"Miscellaneous Utilities.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import errno\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from os import path as osp\n",
        "\n",
        "try:\n",
        "  import pynvml  # nvidia-ml provides utility for NVIDIA management\n",
        "\n",
        "  HAS_NVML = True\n",
        "except:\n",
        "  HAS_NVML = False\n",
        "\n",
        "\n",
        "def auto_select_gpu():\n",
        "  \"\"\"Select gpu which has largest free memory\"\"\"\n",
        "  if HAS_NVML:\n",
        "    pynvml.nvmlInit()\n",
        "    deviceCount = pynvml.nvmlDeviceGetCount()\n",
        "    largest_free_mem = 0\n",
        "    largest_free_idx = 0\n",
        "    for i in range(deviceCount):\n",
        "      handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "      info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "      if info.free > largest_free_mem:\n",
        "        largest_free_mem = info.free\n",
        "        largest_free_idx = i\n",
        "    pynvml.nvmlShutdown()\n",
        "    largest_free_mem = largest_free_mem / 1024. / 1024.  # Convert to MB\n",
        "\n",
        "    idx_to_gpu_id = {}\n",
        "    for i in range(deviceCount):\n",
        "      idx_to_gpu_id[i] = '{}'.format(i)\n",
        "\n",
        "    gpu_id = idx_to_gpu_id[largest_free_idx]\n",
        "    logging.info('Using largest free memory GPU {} with free memory {}MB'.format(gpu_id, largest_free_mem))\n",
        "    return gpu_id\n",
        "  else:\n",
        "    logging.info('nvidia-ml-py is not installed, automatically select gpu is disabled!')\n",
        "    return '0'\n",
        "\n",
        "\n",
        "def get_center(x):\n",
        "  return (x - 1.) / 2.\n",
        "\n",
        "\n",
        "def get(config, key, default):\n",
        "  \"\"\"Get value in config by key, use default if key is not set\n",
        "  This little function is useful for dynamical experimental settings.\n",
        "  For example, we can add a new configuration without worrying compatibility with older versions.\n",
        "  You can also achieve this by just calling config.get(key, default), but add a warning is even better : )\n",
        "  \"\"\"\n",
        "  val = config.get(key)\n",
        "  if val is None:\n",
        "    logging.warning('{} is not explicitly specified, using default value: {}'.format(key, default))\n",
        "    val = default\n",
        "  return val\n",
        "\n",
        "\n",
        "def mkdir_p(path):\n",
        "  \"\"\"mimic the behavior of mkdir -p in bash\"\"\"\n",
        "  try:\n",
        "    os.makedirs(path)\n",
        "  except OSError as exc:  # Python >2.5\n",
        "    if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "      pass\n",
        "    else:\n",
        "      raise\n",
        "\n",
        "\n",
        "def tryfloat(s):\n",
        "  try:\n",
        "    return float(s)\n",
        "  except:\n",
        "    return s\n",
        "\n",
        "\n",
        "def alphanum_key(s):\n",
        "  \"\"\" Turn a string into a list of string and number chunks.\n",
        "      \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "  \"\"\"\n",
        "  return [tryfloat(c) for c in re.split('([0-9.]+)', s)]\n",
        "\n",
        "\n",
        "def sort_nicely(l):\n",
        "  \"\"\"Sort the given list in the way that humans expect.\"\"\"\n",
        "  return sorted(l, key=alphanum_key)\n",
        "\n",
        "\n",
        "class Tee(object):\n",
        "  \"\"\"Mimic the behavior of tee in bash\n",
        "  From: http://web.archive.org/web/20141016185743/https://mail.python.org/pipermail/python-list/2007-May/460639.html\n",
        "  Usage:\n",
        "    tee=Tee('logfile', 'w')\n",
        "    print 'abcdefg'\n",
        "    print 'another line'\n",
        "    tee.close()\n",
        "    print 'screen only'\n",
        "    del tee # should do nothing\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, mode):\n",
        "    self.file = open(name, mode)\n",
        "    self.stdout = sys.stdout\n",
        "    sys.stdout = self\n",
        "\n",
        "  def close(self):\n",
        "    if self.stdout is not None:\n",
        "      sys.stdout = self.stdout\n",
        "      self.stdout = None\n",
        "    if self.file is not None:\n",
        "      self.file.close()\n",
        "      self.file = None\n",
        "\n",
        "  def write(self, data):\n",
        "    self.file.write(data)\n",
        "    self.stdout.write(data)\n",
        "\n",
        "  def flush(self):\n",
        "    self.file.flush()\n",
        "    self.stdout.flush()\n",
        "\n",
        "  def __del__(self):\n",
        "    self.close()\n",
        "\n",
        "\n",
        "def save_cfgs(train_dir, model_config, train_config, track_config):\n",
        "  \"\"\"Save all configurations in JSON format for future reference\"\"\"\n",
        "  with open(osp.join(train_dir, 'model_config.json'), 'w') as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "  with open(osp.join(train_dir, 'train_config.json'), 'w') as f:\n",
        "    json.dump(train_config, f, indent=2)\n",
        "  with open(osp.join(train_dir, 'track_config.json'), 'w') as f:\n",
        "    json.dump(track_config, f, indent=2)\n",
        "\n",
        "\n",
        "def load_cfgs(checkpoint):\n",
        "  if osp.isdir(checkpoint):\n",
        "    train_dir = checkpoint\n",
        "  else:\n",
        "    train_dir = osp.dirname(checkpoint)\n",
        "\n",
        "  with open(osp.join(train_dir, 'model_config.json'), 'r') as f:\n",
        "    model_config = json.load(f)\n",
        "  with open(osp.join(train_dir, 'train_config.json'), 'r') as f:\n",
        "    train_config = json.load(f)\n",
        "  with open(osp.join(train_dir, 'track_config.json'), 'r') as f:\n",
        "    track_config = json.load(f)\n",
        "  return model_config, train_config, track_config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoXGaCZ_3rSu",
        "colab_type": "code",
        "outputId": "29c05635-4d5f-40b6-a695-698cc9063f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNAdq2sL168h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ibBefhz4koC",
        "colab_type": "code",
        "outputId": "51322870-c5dc-437c-8c57-838d8c550d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__ "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mU8ZXWE3y8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"Contains definitions of the network in [1].\n",
        "  [1] Bertinetto, L., et al. (2016).\n",
        "      \"Fully-Convolutional Siamese Networks for Object Tracking.\"\n",
        "      arXiv preprint arXiv:1606.09549.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "\n",
        "def convolutional_alexnet_arg_scope(embed_config,\n",
        "                                    trainable=True,\n",
        "                                    is_training=False):\n",
        "  \"\"\"Defines the default arg scope.\n",
        "  Args:\n",
        "    embed_config: A dictionary which contains configurations for the embedding function.\n",
        "    trainable: If the weights in the embedding function is trainable.\n",
        "    is_training: If the embedding function is built for training.\n",
        "  Returns:\n",
        "    An `arg_scope` to use for the convolutional_alexnet models.\n",
        "  \"\"\"\n",
        "  # Only consider the model to be in training mode if it's trainable.\n",
        "  # This is vital for batch_norm since moving_mean and moving_variance\n",
        "  # will get updated even if not trainable.\n",
        "  is_model_training = trainable and is_training\n",
        "\n",
        "  if get(embed_config, 'use_bn', True):\n",
        "    batch_norm_scale = get(embed_config, 'bn_scale', True)\n",
        "    batch_norm_decay = 1 - get(embed_config, 'bn_momentum', 3e-4)\n",
        "    batch_norm_epsilon = get(embed_config, 'bn_epsilon', 1e-6)\n",
        "    batch_norm_params = {\n",
        "      \"scale\": batch_norm_scale,\n",
        "      # Decay for the moving averages.\n",
        "      \"decay\": batch_norm_decay,\n",
        "      # Epsilon to prevent 0s in variance.\n",
        "      \"epsilon\": batch_norm_epsilon,\n",
        "      \"trainable\": trainable,\n",
        "      \"is_training\": is_model_training,\n",
        "      # Collection containing the moving mean and moving variance.\n",
        "      \"variables_collections\": {\n",
        "        \"beta\": None,\n",
        "        \"gamma\": None,\n",
        "        \"moving_mean\": [\"moving_vars\"],\n",
        "        \"moving_variance\": [\"moving_vars\"],\n",
        "      },\n",
        "      'updates_collections': None,  # Ensure that updates are done within a frame\n",
        "    }\n",
        "    normalizer_fn = slim.batch_norm\n",
        "  else:\n",
        "    batch_norm_params = {}\n",
        "    normalizer_fn = None\n",
        "\n",
        "  weight_decay = get(embed_config, 'weight_decay', 5e-4)\n",
        "  if trainable:\n",
        "    weights_regularizer = slim.l2_regularizer(weight_decay)\n",
        "  else:\n",
        "    weights_regularizer = None\n",
        "\n",
        "  init_method = get(embed_config, 'init_method', 'kaiming_normal')\n",
        "  if is_model_training:\n",
        "    logging.info('embedding init method -- {}'.format(init_method))\n",
        "  if init_method == 'kaiming_normal':\n",
        "    # The same setting as siamese-fc\n",
        "    initializer = slim.variance_scaling_initializer(factor=2.0, mode='FAN_OUT', uniform=False)\n",
        "  else:\n",
        "    initializer = slim.xavier_initializer()\n",
        "\n",
        "  with slim.arg_scope(\n",
        "      [slim.conv2d],\n",
        "      weights_regularizer=weights_regularizer,\n",
        "      weights_initializer=initializer,\n",
        "      padding='VALID',\n",
        "      trainable=trainable,\n",
        "      activation_fn=tf.nn.relu,\n",
        "      normalizer_fn=normalizer_fn,\n",
        "      normalizer_params=batch_norm_params):\n",
        "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
        "      with slim.arg_scope([slim.batch_norm], is_training=is_model_training) as arg_sc:\n",
        "        return arg_sc\n",
        "\n",
        "\n",
        "def convolutional_alexnet(inputs, reuse=None, scope='convolutional_alexnet'):\n",
        "  \"\"\"Defines the feature extractor of SiamFC.\n",
        "  Args:\n",
        "    inputs: a Tensor of shape [batch, h, w, c].\n",
        "    reuse: if the weights in the embedding function are reused.\n",
        "    scope: the variable scope of the computational graph.\n",
        "  Returns:\n",
        "    net: the computed features of the inputs.\n",
        "    end_points: the intermediate outputs of the embedding function.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope, 'convolutional_alexnet', [inputs], reuse=reuse) as sc:\n",
        "    end_points_collection = sc.name + '_end_points'\n",
        "    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n",
        "                        outputs_collections=end_points_collection):\n",
        "      net = inputs\n",
        "      net = slim.conv2d(net, 96, [11, 11], 2, scope='conv1')\n",
        "      net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')\n",
        "      with tf.variable_scope('conv2'):\n",
        "        b1, b2 = tf.split(net, 2, 3)\n",
        "        b1 = slim.conv2d(b1, 128, [5, 5], scope='b1')\n",
        "        # The original implementation has bias terms for all convolution, but\n",
        "        # it actually isn't necessary if the convolution layer is followed by a batch\n",
        "        # normalization layer since batch norm will subtract the mean.\n",
        "        b2 = slim.conv2d(b2, 128, [5, 5], scope='b2')\n",
        "        net = tf.concat([b1, b2], 3)\n",
        "      net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\n",
        "      net = slim.conv2d(net, 384, [3, 3], 1, scope='conv3')\n",
        "      with tf.variable_scope('conv4'):\n",
        "        b1, b2 = tf.split(net, 2, 3)\n",
        "        b1 = slim.conv2d(b1, 192, [3, 3], 1, scope='b1')\n",
        "        b2 = slim.conv2d(b2, 192, [3, 3], 1, scope='b2')\n",
        "        net = tf.concat([b1, b2], 3)\n",
        "      # Conv 5 with only convolution, has bias\n",
        "      with tf.variable_scope('conv5'):\n",
        "        with slim.arg_scope([slim.conv2d],\n",
        "                            activation_fn=None, normalizer_fn=None):\n",
        "          b1, b2 = tf.split(net, 2, 3)\n",
        "          b1 = slim.conv2d(b1, 128, [3, 3], 1, scope='b1')\n",
        "          b2 = slim.conv2d(b2, 128, [3, 3], 1, scope='b2')\n",
        "        net = tf.concat([b1, b2], 3)\n",
        "      # Convert end_points_collection into a dictionary of end_points.\n",
        "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
        "      return net, end_points\n",
        "\n",
        "\n",
        "convolutional_alexnet.stride = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VMMLf8FxbmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"Utilities for model construction\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy import io as sio\n",
        "\n",
        "\n",
        "\n",
        "def construct_gt_score_maps(response_size, batch_size, stride, gt_config=None):\n",
        "  \"\"\"Construct a batch of groundtruth score maps\n",
        "  Args:\n",
        "    response_size: A list or tuple with two elements [ho, wo]\n",
        "    batch_size: An integer e.g., 16\n",
        "    stride: Embedding stride e.g., 8\n",
        "    gt_config: Configurations for groundtruth generation\n",
        "  Return:\n",
        "    A float tensor of shape [batch_size] + response_size\n",
        "  \"\"\"\n",
        "  with tf.name_scope('construct_gt'):\n",
        "    ho = response_size[0]\n",
        "    wo = response_size[1]\n",
        "    y = tf.cast(tf.range(0, ho), dtype=tf.float32) - get_center(ho)\n",
        "    x = tf.cast(tf.range(0, wo), dtype=tf.float32) - get_center(wo)\n",
        "    [Y, X] = tf.meshgrid(y, x)\n",
        "\n",
        "    def _logistic_label(X, Y, rPos, rNeg):\n",
        "      # dist_to_center = tf.sqrt(tf.square(X) + tf.square(Y))  # L2 metric\n",
        "      dist_to_center = tf.abs(X) + tf.abs(Y)  # Block metric\n",
        "      Z = tf.where(dist_to_center <= rPos,\n",
        "                   tf.ones_like(X),\n",
        "                   tf.where(dist_to_center < rNeg,\n",
        "                            0.5 * tf.ones_like(X),\n",
        "                            tf.zeros_like(X)))\n",
        "      return Z\n",
        "\n",
        "    rPos = gt_config['rPos'] / stride\n",
        "    rNeg = gt_config['rNeg'] / stride\n",
        "    gt = _logistic_label(X, Y, rPos, rNeg)\n",
        "\n",
        "    # Duplicate a batch of maps\n",
        "    gt_expand = tf.reshape(gt, [1] + response_size)\n",
        "    gt = tf.tile(gt_expand, [batch_size, 1, 1])\n",
        "    return gt\n",
        "\n",
        "\n",
        "def get_params_from_mat(matpath):\n",
        "  \"\"\"Get parameter from .mat file into parms(dict)\"\"\"\n",
        "\n",
        "  def squeeze(vars_):\n",
        "    # Matlab save some params with shape (*, 1)\n",
        "    # However, we don't need the trailing dimension in TensorFlow.\n",
        "    if isinstance(vars_, (list, tuple)):\n",
        "      return [np.squeeze(v, 1) for v in vars_]\n",
        "    else:\n",
        "      return np.squeeze(vars_, 1)\n",
        "\n",
        "  netparams = sio.loadmat(matpath)[\"net\"][\"params\"][0][0]\n",
        "  params = dict()\n",
        "\n",
        "  for i in range(netparams.size):\n",
        "    param = netparams[0][i]\n",
        "    name = param[\"name\"][0]\n",
        "    value = param[\"value\"]\n",
        "    value_size = param[\"value\"].shape[0]\n",
        "\n",
        "    match = re.match(r\"([a-z]+)([0-9]+)([a-z]+)\", name, re.I)\n",
        "    if match:\n",
        "      items = match.groups()\n",
        "    elif name == 'adjust_f':\n",
        "      params['detection/weights'] = squeeze(value)\n",
        "      continue\n",
        "    elif name == 'adjust_b':\n",
        "      params['detection/biases'] = squeeze(value)\n",
        "      continue\n",
        "    else:\n",
        "      raise Exception('unrecognized layer params')\n",
        "\n",
        "    op, layer, types = items\n",
        "    layer = int(layer)\n",
        "    if layer in [1, 3]:\n",
        "      if op == 'conv':  # convolution\n",
        "        if types == 'f':\n",
        "          params['conv%d/weights' % layer] = value\n",
        "        elif types == 'b':\n",
        "          value = squeeze(value)\n",
        "          params['conv%d/biases' % layer] = value\n",
        "      elif op == 'bn':  # batch normalization\n",
        "        if types == 'x':\n",
        "          m, v = squeeze(np.split(value, 2, 1))\n",
        "          params['conv%d/BatchNorm/moving_mean' % layer] = m\n",
        "          params['conv%d/BatchNorm/moving_variance' % layer] = np.square(v)\n",
        "        elif types == 'm':\n",
        "          value = squeeze(value)\n",
        "          params['conv%d/BatchNorm/gamma' % layer] = value\n",
        "        elif types == 'b':\n",
        "          value = squeeze(value)\n",
        "          params['conv%d/BatchNorm/beta' % layer] = value\n",
        "      else:\n",
        "        raise Exception\n",
        "    elif layer in [2, 4]:\n",
        "      if op == 'conv' and types == 'f':\n",
        "        b1, b2 = np.split(value, 2, 3)\n",
        "      else:\n",
        "        b1, b2 = np.split(value, 2, 0)\n",
        "      if op == 'conv':\n",
        "        if types == 'f':\n",
        "          params['conv%d/b1/weights' % layer] = b1\n",
        "          params['conv%d/b2/weights' % layer] = b2\n",
        "        elif types == 'b':\n",
        "          b1, b2 = squeeze(np.split(value, 2, 0))\n",
        "          params['conv%d/b1/biases' % layer] = b1\n",
        "          params['conv%d/b2/biases' % layer] = b2\n",
        "      elif op == 'bn':\n",
        "        if types == 'x':\n",
        "          m1, v1 = squeeze(np.split(b1, 2, 1))\n",
        "          m2, v2 = squeeze(np.split(b2, 2, 1))\n",
        "          params['conv%d/b1/BatchNorm/moving_mean' % layer] = m1\n",
        "          params['conv%d/b2/BatchNorm/moving_mean' % layer] = m2\n",
        "          params['conv%d/b1/BatchNorm/moving_variance' % layer] = np.square(v1)\n",
        "          params['conv%d/b2/BatchNorm/moving_variance' % layer] = np.square(v2)\n",
        "        elif types == 'm':\n",
        "          params['conv%d/b1/BatchNorm/gamma' % layer] = squeeze(b1)\n",
        "          params['conv%d/b2/BatchNorm/gamma' % layer] = squeeze(b2)\n",
        "        elif types == 'b':\n",
        "          params['conv%d/b1/BatchNorm/beta' % layer] = squeeze(b1)\n",
        "          params['conv%d/b2/BatchNorm/beta' % layer] = squeeze(b2)\n",
        "      else:\n",
        "        raise Exception\n",
        "\n",
        "    elif layer in [5]:\n",
        "      if op == 'conv' and types == 'f':\n",
        "        b1, b2 = np.split(value, 2, 3)\n",
        "      else:\n",
        "        b1, b2 = squeeze(np.split(value, 2, 0))\n",
        "      assert op == 'conv', 'layer5 contains only convolution'\n",
        "      if types == 'f':\n",
        "        params['conv%d/b1/weights' % layer] = b1\n",
        "        params['conv%d/b2/weights' % layer] = b2\n",
        "      elif types == 'b':\n",
        "        params['conv%d/b1/biases' % layer] = b1\n",
        "        params['conv%d/b2/biases' % layer] = b2\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def load_mat_model(matpath, embed_scope, detection_scope=None):\n",
        "  \"\"\"Restore SiameseFC models from .mat model files\"\"\"\n",
        "  params = get_params_from_mat(matpath)\n",
        "\n",
        "  assign_ops = []\n",
        "\n",
        "  def _assign(ref_name, params, scope=embed_scope):\n",
        "    var_in_model = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                     scope + ref_name)[0]\n",
        "    var_in_mat = params[ref_name]\n",
        "    op = tf.assign(var_in_model, var_in_mat)\n",
        "    assign_ops.append(op)\n",
        "\n",
        "  for l in range(1, 6):\n",
        "    if l in [1, 3]:\n",
        "      _assign('conv%d/weights' % l, params)\n",
        "      # _assign('conv%d/biases' % l, params)\n",
        "      _assign('conv%d/BatchNorm/beta' % l, params)\n",
        "      _assign('conv%d/BatchNorm/gamma' % l, params)\n",
        "      _assign('conv%d/BatchNorm/moving_mean' % l, params)\n",
        "      _assign('conv%d/BatchNorm/moving_variance' % l, params)\n",
        "    elif l in [2, 4]:\n",
        "      # Branch 1\n",
        "      _assign('conv%d/b1/weights' % l, params)\n",
        "      # _assign('conv%d/b1/biases' % l, params)\n",
        "      _assign('conv%d/b1/BatchNorm/beta' % l, params)\n",
        "      _assign('conv%d/b1/BatchNorm/gamma' % l, params)\n",
        "      _assign('conv%d/b1/BatchNorm/moving_mean' % l, params)\n",
        "      _assign('conv%d/b1/BatchNorm/moving_variance' % l, params)\n",
        "      # Branch 2\n",
        "      _assign('conv%d/b2/weights' % l, params)\n",
        "      # _assign('conv%d/b2/biases' % l, params)\n",
        "      _assign('conv%d/b2/BatchNorm/beta' % l, params)\n",
        "      _assign('conv%d/b2/BatchNorm/gamma' % l, params)\n",
        "      _assign('conv%d/b2/BatchNorm/moving_mean' % l, params)\n",
        "      _assign('conv%d/b2/BatchNorm/moving_variance' % l, params)\n",
        "    elif l in [5]:\n",
        "      # Branch 1\n",
        "      _assign('conv%d/b1/weights' % l, params)\n",
        "      _assign('conv%d/b1/biases' % l, params)\n",
        "      # Branch 2\n",
        "      _assign('conv%d/b2/weights' % l, params)\n",
        "      _assign('conv%d/b2/biases' % l, params)\n",
        "    else:\n",
        "      raise Exception('layer number must below 5')\n",
        "\n",
        "  if detection_scope:\n",
        "    _assign(detection_scope + 'biases', params, scope='')\n",
        "\n",
        "  initialize = tf.group(*assign_ops)\n",
        "  return initialize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdznsVczxmdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.ops.metrics_impl import _confusion_matrix_at_thresholds\n",
        "\n",
        "\n",
        "def _auc(labels, predictions, weights=None, num_thresholds=200,\n",
        "         metrics_collections=None, updates_collections=None,\n",
        "         curve='ROC', name=None, summation_method='trapezoidal'):\n",
        "  \"\"\"Computes the approximate AUC via a Riemann sum.\n",
        "  Modified version of tf.metrics.auc. Add support for AUC computation\n",
        "  of the recall curve.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(\n",
        "      name, 'auc', (labels, predictions, weights)):\n",
        "    if curve != 'ROC' and curve != 'PR' and curve != 'R':\n",
        "      raise ValueError('curve must be either ROC, PR or R, %s unknown' %\n",
        "                       (curve))\n",
        "    kepsilon = 1e-7  # to account for floating point imprecisions\n",
        "    thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n",
        "                  for i in range(num_thresholds - 2)]\n",
        "    thresholds = [0.0 - kepsilon] + thresholds + [1.0 + kepsilon]\n",
        "\n",
        "    values, update_ops = _confusion_matrix_at_thresholds(\n",
        "      labels, predictions, thresholds, weights)\n",
        "\n",
        "    # Add epsilons to avoid dividing by 0.\n",
        "    epsilon = 1.0e-6\n",
        "\n",
        "    def compute_auc(tp, fn, tn, fp, name):\n",
        "      \"\"\"Computes the roc-auc or pr-auc based on confusion counts.\"\"\"\n",
        "      rec = tf.div(tp + epsilon, tp + fn + epsilon)\n",
        "      if curve == 'ROC':\n",
        "        fp_rate = tf.div(fp, fp + tn + epsilon)\n",
        "        x = fp_rate\n",
        "        y = rec\n",
        "      elif curve == 'R':  # recall auc\n",
        "        x = tf.linspace(1., 0., num_thresholds)\n",
        "        y = rec\n",
        "      else:  # curve == 'PR'.\n",
        "        prec = tf.div(tp + epsilon, tp + fp + epsilon)\n",
        "        x = rec\n",
        "        y = prec\n",
        "      if summation_method == 'trapezoidal':\n",
        "        return tf.reduce_sum(\n",
        "          tf.multiply(x[:num_thresholds - 1] - x[1:],\n",
        "                      (y[:num_thresholds - 1] + y[1:]) / 2.),\n",
        "          name=name)\n",
        "      elif summation_method == 'minoring':\n",
        "        return tf.reduce_sum(\n",
        "          tf.multiply(x[:num_thresholds - 1] - x[1:],\n",
        "                      tf.minimum(y[:num_thresholds - 1], y[1:])),\n",
        "          name=name)\n",
        "      elif summation_method == 'majoring':\n",
        "        return tf.reduce_sum(\n",
        "          tf.multiply(x[:num_thresholds - 1] - x[1:],\n",
        "                      tf.maximum(y[:num_thresholds - 1], y[1:])),\n",
        "          name=name)\n",
        "      else:\n",
        "        raise ValueError('Invalid summation_method: %s' % summation_method)\n",
        "\n",
        "    # sum up the areas of all the trapeziums\n",
        "    auc_value = compute_auc(\n",
        "      values['tp'], values['fn'], values['tn'], values['fp'], 'value')\n",
        "    update_op = compute_auc(\n",
        "      update_ops['tp'], update_ops['fn'], update_ops['tn'], update_ops['fp'],\n",
        "      'update_op')\n",
        "\n",
        "    if metrics_collections:\n",
        "      ops.add_to_collections(metrics_collections, auc_value)\n",
        "\n",
        "    if updates_collections:\n",
        "      ops.add_to_collections(updates_collections, update_op)\n",
        "\n",
        "    return auc_value, update_op\n",
        "\n",
        "\n",
        "def get_center_index(response):\n",
        "  \"\"\"Get the index of the center in the response map\"\"\"\n",
        "  shape = tf.shape(response)\n",
        "  c1 = tf.to_int32((shape[1] - 1) / 2)\n",
        "  c2 = tf.to_int32((shape[2] - 1) / 2)\n",
        "  return c1, c2\n",
        "\n",
        "\n",
        "def center_score_error(response):\n",
        "  \"\"\"Center score error.\n",
        "  The error is low when the center of the response map is classified as target.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('CS-err'):\n",
        "    r, c = get_center_index(response)\n",
        "    center_score = response[:, r, c]\n",
        "    mean, update_op = tf.metrics.mean(tf.to_float(center_score < 0))\n",
        "    with tf.control_dependencies([update_op]):\n",
        "      mean = tf.identity(mean)\n",
        "    return mean\n",
        "\n",
        "\n",
        "def get_maximum_index(response):\n",
        "  \"\"\"Get the index of the maximum value in the response map\"\"\"\n",
        "  response_shape = response.get_shape().as_list()\n",
        "  response_spatial_size = response_shape[-2:]  # e.g. [29, 29]\n",
        "  length = response_spatial_size[0] * response_spatial_size[1]\n",
        "\n",
        "  # Get maximum response index (note index starts from zero)\n",
        "  ind_max = tf.argmax(tf.reshape(response, [-1, length]), 1)\n",
        "  ind_row = tf.div(ind_max, response_spatial_size[1])\n",
        "  ind_col = tf.mod(ind_max, response_spatial_size[1])\n",
        "  return ind_row, ind_col\n",
        "\n",
        "\n",
        "def center_dist_error(response):\n",
        "  \"\"\"Center distance error.\n",
        "  The error is low when the maximum response is at the center of the response map.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('CD-err'):\n",
        "    radius_in_pixel = 50.\n",
        "    total_stride = 8.\n",
        "    num_thresholds = 100\n",
        "    radius_in_response = radius_in_pixel / total_stride\n",
        "\n",
        "    gt_r, gt_c = get_center_index(response)\n",
        "    max_r, max_c = get_maximum_index(response)\n",
        "    gt_r = tf.to_float(gt_r)\n",
        "    gt_c = tf.to_float(gt_c)\n",
        "    max_r = tf.to_float(max_r)\n",
        "    max_c = tf.to_float(max_c)\n",
        "    distances = tf.sqrt((gt_r - max_r) ** 2 + (gt_c - max_c) ** 2)\n",
        "\n",
        "    # We cast distances as prediction accuracies in the range [0, 1] where 0 means fail and\n",
        "    # 1 means success. In this way, we can readily use streaming_auc to compute area\n",
        "    # under curve.\n",
        "    dist_norm = distances / radius_in_response\n",
        "    dist_norm = tf.minimum(dist_norm, 1.)\n",
        "    predictions = 1. - dist_norm\n",
        "    labels = tf.ones_like(predictions)\n",
        "\n",
        "    auc, update_op = _auc(labels, predictions, num_thresholds=num_thresholds, curve='R')\n",
        "    with tf.control_dependencies([update_op]):\n",
        "      err = 1. - auc\n",
        "    return err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWb2i1sDyCuP",
        "colab_type": "code",
        "outputId": "f923c77f-7376-4a35-f190-77c5e838f68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"Dataset Sampler\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Sampler(object):\n",
        "  def __init__(self, data_source, shuffle=True):\n",
        "    self.data_source = data_source\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "  def __iter__(self):\n",
        "    data_idxs = np.arange(len(self.data_source))\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(data_idxs)\n",
        "\n",
        "    for idx in data_idxs:\n",
        "      yield idx\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  x = [1, 2, 3]\n",
        "  sampler = Sampler(x, shuffle=True)\n",
        "  p = 0\n",
        "  for xx in sampler:\n",
        "    print(x[xx])\n",
        "    p += 1\n",
        "    if p == 10: break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hume9y2j0fi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\n",
        "\"\"\"Various transforms for video and image augmentation\"\"\"\n",
        "\n",
        "import numbers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Compose(object):\n",
        "  \"\"\"Composes several transforms together.\"\"\"\n",
        "\n",
        "  def __init__(self, transforms):\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __call__(self, example):\n",
        "    for t in self.transforms:\n",
        "      example = t(example)\n",
        "    return example\n",
        "\n",
        "\n",
        "class RandomGray(object):\n",
        "  def __init__(self, gray_ratio=0.25):\n",
        "    self.gray_ratio = gray_ratio\n",
        "\n",
        "  def __call__(self, img_sequence):\n",
        "    def rgb_to_gray():\n",
        "      gray_images = tf.image.rgb_to_grayscale(img_sequence)\n",
        "      return tf.concat([gray_images] * 3, axis=3)\n",
        "\n",
        "    def identity():\n",
        "      return tf.identity(img_sequence)\n",
        "\n",
        "    return tf.cond(tf.less(tf.random_uniform([], 0, 1), self.gray_ratio), rgb_to_gray, identity)\n",
        "\n",
        "\n",
        "class RandomStretch(object):\n",
        "  def __init__(self, max_stretch=0.05, interpolation='bilinear'):\n",
        "    self.max_stretch = max_stretch\n",
        "    self.interpolation = interpolation\n",
        "\n",
        "  def __call__(self, img):\n",
        "    scale = 1.0 + tf.random_uniform([], -self.max_stretch, self.max_stretch)\n",
        "    img_shape = tf.shape(img)\n",
        "    ts = tf.to_int32(tf.round(tf.to_float(img_shape[:2]) * scale))\n",
        "    resize_method_map = {'bilinear': tf.image.ResizeMethod.BILINEAR,\n",
        "                         'bicubic': tf.image.ResizeMethod.BICUBIC}\n",
        "    return tf.image.resize_images(img, ts, method=resize_method_map[self.interpolation])\n",
        "\n",
        "\n",
        "class CenterCrop(object):\n",
        "  def __init__(self, size):\n",
        "    if isinstance(size, numbers.Number):\n",
        "      self.size = (int(size), int(size))\n",
        "    else:\n",
        "      self.size = size\n",
        "\n",
        "  def __call__(self, img):\n",
        "    th, tw = self.size\n",
        "    return tf.image.resize_image_with_crop_or_pad(img, th, tw)\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "  def __init__(self, size):\n",
        "    if isinstance(size, numbers.Number):\n",
        "      self.size = (int(size), int(size))\n",
        "    else:\n",
        "      self.size = size\n",
        "\n",
        "  def __call__(self, img):\n",
        "    img_shape = tf.shape(img)\n",
        "    th, tw = self.size\n",
        "\n",
        "    y1 = tf.random_uniform([], 0, img_shape[0] - th, dtype=tf.int32)\n",
        "    x1 = tf.random_uniform([], 0, img_shape[1] - tw, dtype=tf.int32)\n",
        "\n",
        "    return tf.image.crop_to_bounding_box(img, y1, x1, th, tw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I6iMO0w06O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"VID Dataset\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def downsample(n_in, n_out, max_frame_dist=1):\n",
        "  # Get a list of frame distance between consecutive frames\n",
        "  max_frame_dist = np.minimum(n_in, max_frame_dist)\n",
        "  possible_frame_dist = range(1, max_frame_dist + 1)\n",
        "  frame_dist = np.random.choice(possible_frame_dist, n_out - 1)\n",
        "  end_to_start_frame_dist = np.sum(frame_dist)\n",
        "\n",
        "  # Check frame dist boundary\n",
        "  possible_max_start_idx = n_in - 1 - end_to_start_frame_dist\n",
        "  if possible_max_start_idx < 0:\n",
        "    n_extra = - possible_max_start_idx\n",
        "    while n_extra > 0:\n",
        "      for idx, dist in enumerate(frame_dist):\n",
        "        if dist > 1:\n",
        "          frame_dist[idx] = dist - 1\n",
        "          n_extra -= 1\n",
        "          if n_extra == 0: break\n",
        "\n",
        "  # Get frame dist\n",
        "  end_to_start_frame_dist = np.sum(frame_dist)\n",
        "  possible_max_start_idx = n_in - 1 - end_to_start_frame_dist\n",
        "  start_idx = np.random.choice(possible_max_start_idx + 1, 1)\n",
        "  out_idxs = np.cumsum(np.concatenate((start_idx, frame_dist)))\n",
        "  return out_idxs\n",
        "\n",
        "\n",
        "def upsample(n_in, n_out):\n",
        "  n_more = n_out - n_in\n",
        "  in_idxs = range(n_in)\n",
        "  more_idxs = np.random.choice(in_idxs, n_more)\n",
        "  out_idxs = sorted(list(in_idxs) + list(more_idxs))\n",
        "  return out_idxs\n",
        "\n",
        "\n",
        "class VID:\n",
        "  def __init__(self, imdb_path, max_frame_dist, epoch_size=None):\n",
        "    with open(imdb_path, 'rb') as f:\n",
        "      imdb = pickle.load(f)\n",
        "\n",
        "    self.videos = imdb['videos']\n",
        "    self.time_steps = 2\n",
        "    self.max_frame_dist = max_frame_dist\n",
        "\n",
        "    if epoch_size is None:\n",
        "      self.epoch_size = len(self.videos)\n",
        "    else:\n",
        "      self.epoch_size = int(epoch_size)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_ids = self.videos[index % len(self.videos)]\n",
        "    n_frames = len(img_ids)\n",
        "\n",
        "    if n_frames < self.time_steps:\n",
        "      out_idxs = upsample(n_frames, self.time_steps)\n",
        "    elif n_frames == self.time_steps:\n",
        "      out_idxs = range(n_frames)\n",
        "    else:\n",
        "      out_idxs = downsample(n_frames, self.time_steps, self.max_frame_dist)\n",
        "\n",
        "    video = []\n",
        "    for j, frame_idx in enumerate(out_idxs):\n",
        "      img_path = img_ids[frame_idx]\n",
        "      video.append(img_path.encode('utf-8'))\n",
        "    return video\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.epoch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMY4l8761HQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "  def __init__(self, config, is_training):\n",
        "    self.config = config\n",
        "    self.is_training = is_training\n",
        "\n",
        "    preprocess_name = get(config, 'preprocessing_name', None)\n",
        "    logging.info('preproces -- {}'.format(preprocess_name))\n",
        "\n",
        "    if preprocess_name == 'siamese_fc_color':\n",
        "      self.v_transform = None\n",
        "      # TODO: use a single operation (tf.image.crop_and_resize) to achieve all transformations ?\n",
        "      self.z_transform = Compose([RandomStretch(),\n",
        "                                  CenterCrop((255 - 8, 255 - 8)),\n",
        "                                  RandomCrop(255 - 2 * 8),\n",
        "                                  CenterCrop((127, 127))])\n",
        "      self.x_transform = Compose([RandomStretch(),\n",
        "                                  CenterCrop((255 - 8, 255 - 8)),\n",
        "                                  RandomCrop(255 - 2 * 8), ])\n",
        "    elif preprocess_name == 'siamese_fc_gray':\n",
        "      self.v_transform = RandomGray()\n",
        "      self.z_transform = Compose([RandomStretch(),\n",
        "                                  CenterCrop((255 - 8, 255 - 8)),\n",
        "                                  RandomCrop(255 - 2 * 8),\n",
        "                                  CenterCrop((127, 127))])\n",
        "      self.x_transform = Compose([RandomStretch(),\n",
        "                                  CenterCrop((255 - 8, 255 - 8)),\n",
        "                                  RandomCrop(255 - 2 * 8), ])\n",
        "    elif preprocess_name == 'None':\n",
        "      self.v_transform = None\n",
        "      self.z_transform = CenterCrop((127, 127))\n",
        "      self.x_transform = CenterCrop((255, 255))\n",
        "    else:\n",
        "      raise ValueError('Preprocessing name {} was not recognized.'.format(preprocess_name))\n",
        "\n",
        "    self.dataset_py = VID(config['input_imdb'], config['max_frame_dist'])\n",
        "    self.sampler = Sampler(self.dataset_py, shuffle=is_training)\n",
        "\n",
        "  def build(self):\n",
        "    self.build_dataset()\n",
        "    self.build_iterator()\n",
        "\n",
        "  def build_dataset(self):\n",
        "    def sample_generator():\n",
        "      for video_id in self.sampler:\n",
        "        sample = self.dataset_py[video_id]\n",
        "        yield sample\n",
        "\n",
        "    def transform_fn(video):\n",
        "      exemplar_file = tf.read_file(video[0])\n",
        "      instance_file = tf.read_file(video[1])\n",
        "      exemplar_image = tf.image.decode_jpeg(exemplar_file, channels=3, dct_method=\"INTEGER_ACCURATE\")\n",
        "      instance_image = tf.image.decode_jpeg(instance_file, channels=3, dct_method=\"INTEGER_ACCURATE\")\n",
        "\n",
        "      if self.v_transform is not None:\n",
        "        video = tf.stack([exemplar_image, instance_image])\n",
        "        video = self.v_transform(video)\n",
        "        exemplar_image = video[0]\n",
        "        instance_image = video[1]\n",
        "\n",
        "      if self.z_transform is not None:\n",
        "        exemplar_image = self.z_transform(exemplar_image)\n",
        "\n",
        "      if self.x_transform is not None:\n",
        "        instance_image = self.x_transform(instance_image)\n",
        "\n",
        "      return exemplar_image, instance_image\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(sample_generator,\n",
        "                                             output_types=(tf.string),\n",
        "                                             output_shapes=(tf.TensorShape([2])))\n",
        "    dataset = dataset.map(transform_fn, num_parallel_calls=self.config['prefetch_threads'])\n",
        "    dataset = dataset.prefetch(self.config['prefetch_capacity'])\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(self.config['batch_size'])\n",
        "    self.dataset_tf = dataset\n",
        "\n",
        "  def build_iterator(self):\n",
        "    self.iterator = self.dataset_tf.make_one_shot_iterator()\n",
        "\n",
        "  def get_one_batch(self):\n",
        "    return self.iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-phA6L1WWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"Construct the computational graph of siamese model for training. \"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import functools\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "\n",
        "class SiameseModel:\n",
        "  def __init__(self, model_config, train_config, mode='train'):\n",
        "    self.model_config = model_config\n",
        "    self.train_config = train_config\n",
        "    self.mode = mode\n",
        "    assert mode in ['train', 'validation', 'inference']\n",
        "\n",
        "    if self.mode == 'train':\n",
        "      self.data_config = self.train_config['train_data_config']\n",
        "    elif self.mode == 'validation':\n",
        "      self.data_config = self.train_config['validation_data_config']\n",
        "\n",
        "    self.dataloader = None\n",
        "    self.exemplars = None\n",
        "    self.instances = None\n",
        "    self.response = None\n",
        "    self.batch_loss = None\n",
        "    self.total_loss = None\n",
        "    self.init_fn = None\n",
        "    self.global_step = None\n",
        "\n",
        "  def is_training(self):\n",
        "    \"\"\"Returns true if the model is built for training mode\"\"\"\n",
        "    return self.mode == 'train'\n",
        "\n",
        "  def build_inputs(self):\n",
        "    \"\"\"Input fetching and batching\n",
        "    Outputs:\n",
        "      self.exemplars: image batch of shape [batch, hz, wz, 3]\n",
        "      self.instances: image batch of shape [batch, hx, wx, 3]\n",
        "    \"\"\"\n",
        "    if self.mode in ['train', 'validation']:\n",
        "      with tf.device(\"/cpu:0\"):  # Put data loading and preprocessing in CPU is substantially faster\n",
        "        self.dataloader = DataLoader(self.data_config, self.is_training())\n",
        "        self.dataloader.build()\n",
        "        exemplars, instances = self.dataloader.get_one_batch()\n",
        "\n",
        "        exemplars = tf.to_float(exemplars)\n",
        "        instances = tf.to_float(instances)\n",
        "    else:\n",
        "      self.examplar_feed = tf.placeholder(shape=[None, None, None, 3],\n",
        "                                          dtype=tf.uint8,\n",
        "                                          name='examplar_input')\n",
        "      self.instance_feed = tf.placeholder(shape=[None, None, None, 3],\n",
        "                                          dtype=tf.uint8,\n",
        "                                          name='instance_input')\n",
        "      exemplars = tf.to_float(self.examplar_feed)\n",
        "      instances = tf.to_float(self.instance_feed)\n",
        "\n",
        "    self.exemplars = exemplars\n",
        "    self.instances = instances\n",
        "\n",
        "  def build_image_embeddings(self, reuse=False):\n",
        "    \"\"\"Builds the image model subgraph and generates image embeddings\n",
        "    Inputs:\n",
        "      self.exemplars: A tensor of shape [batch, hz, wz, 3]\n",
        "      self.instances: A tensor of shape [batch, hx, wx, 3]\n",
        "    Outputs:\n",
        "      self.exemplar_embeds: A Tensor of shape [batch, hz_embed, wz_embed, embed_dim]\n",
        "      self.instance_embeds: A Tensor of shape [batch, hx_embed, wx_embed, embed_dim]\n",
        "    \"\"\"\n",
        "    config = self.model_config['embed_config']\n",
        "    arg_scope = convolutional_alexnet_arg_scope(config,\n",
        "                                                trainable=config['train_embedding'],\n",
        "                                                is_training=self.is_training())\n",
        "\n",
        "    @functools.wraps(convolutional_alexnet)\n",
        "    def embedding_fn(images, reuse=False):\n",
        "      with slim.arg_scope(arg_scope):\n",
        "        return convolutional_alexnet(images, reuse=reuse)\n",
        "\n",
        "    self.exemplar_embeds, _ = embedding_fn(self.exemplars, reuse=reuse)\n",
        "    self.instance_embeds, _ = embedding_fn(self.instances, reuse=True)\n",
        "\n",
        "  def build_template(self):\n",
        "    # The template is simply the feature of the exemplar image in SiamFC.\n",
        "    self.templates = self.exemplar_embeds\n",
        "\n",
        "  def build_detection(self, reuse=False):\n",
        "    with tf.variable_scope('detection', reuse=reuse):\n",
        "      def _translation_match(x, z):  # translation match for one example within a batch\n",
        "        x = tf.expand_dims(x, 0)  # [1, in_height, in_width, in_channels]\n",
        "        z = tf.expand_dims(z, -1)  # [filter_height, filter_width, in_channels, 1]\n",
        "        return tf.nn.conv2d(x, z, strides=[1, 1, 1, 1], padding='VALID', name='translation_match')\n",
        "\n",
        "      output = tf.map_fn(lambda x: _translation_match(x[0], x[1]),\n",
        "                         (self.instance_embeds, self.templates),\n",
        "                         dtype=self.instance_embeds.dtype)\n",
        "      output = tf.squeeze(output, [1, 4])  # of shape e.g., [8, 15, 15]\n",
        "\n",
        "      # Adjust score, this is required to make training possible.\n",
        "      config = self.model_config['adjust_response_config']\n",
        "      bias = tf.get_variable('biases', [1],\n",
        "                             dtype=tf.float32,\n",
        "                             initializer=tf.constant_initializer(0.0, dtype=tf.float32),\n",
        "                             trainable=config['train_bias'])\n",
        "      response = config['scale'] * output + bias\n",
        "      self.response = response\n",
        "\n",
        "  def build_loss(self):\n",
        "    response = self.response\n",
        "    response_size = response.get_shape().as_list()[1:3]  # [height, width]\n",
        "\n",
        "    gt = construct_gt_score_maps(response_size,\n",
        "                                 self.data_config['batch_size'],\n",
        "                                 self.model_config['embed_config']['stride'],\n",
        "                                 self.train_config['gt_config'])\n",
        "\n",
        "    with tf.name_scope('Loss'):\n",
        "      loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=response,\n",
        "                                                     labels=gt)\n",
        "\n",
        "      with tf.name_scope('Balance_weights'):\n",
        "        n_pos = tf.reduce_sum(tf.to_float(tf.equal(gt[0], 1)))\n",
        "        n_neg = tf.reduce_sum(tf.to_float(tf.equal(gt[0], 0)))\n",
        "        w_pos = 0.5 / n_pos\n",
        "        w_neg = 0.5 / n_neg\n",
        "        class_weights = tf.where(tf.equal(gt, 1),\n",
        "                                 w_pos * tf.ones_like(gt),\n",
        "                                 tf.ones_like(gt))\n",
        "        class_weights = tf.where(tf.equal(gt, 0),\n",
        "                                 w_neg * tf.ones_like(gt),\n",
        "                                 class_weights)\n",
        "        loss = loss * class_weights\n",
        "\n",
        "      # Note that we use reduce_sum instead of reduce_mean since the loss has\n",
        "      # already been normalized by class_weights in spatial dimension.\n",
        "      loss = tf.reduce_sum(loss, [1, 2])\n",
        "\n",
        "      batch_loss = tf.reduce_mean(loss, name='batch_loss')\n",
        "      tf.losses.add_loss(batch_loss)\n",
        "\n",
        "      total_loss = tf.losses.get_total_loss()\n",
        "      self.batch_loss = batch_loss\n",
        "      self.total_loss = total_loss\n",
        "\n",
        "      tf.summary.image('exemplar', self.exemplars, family=self.mode)\n",
        "      tf.summary.image('instance', self.instances, family=self.mode)\n",
        "\n",
        "      mean_batch_loss, update_op1 = tf.metrics.mean(batch_loss)\n",
        "      mean_total_loss, update_op2 = tf.metrics.mean(total_loss)\n",
        "      with tf.control_dependencies([update_op1, update_op2]):\n",
        "        tf.summary.scalar('batch_loss', mean_batch_loss, family=self.mode)\n",
        "        tf.summary.scalar('total_loss', mean_total_loss, family=self.mode)\n",
        "\n",
        "      if self.mode == 'train':\n",
        "        tf.summary.image('GT', tf.reshape(gt[0], [1] + response_size + [1]), family='GT')\n",
        "      tf.summary.image('Response', tf.expand_dims(tf.sigmoid(response), -1), family=self.mode)\n",
        "      tf.summary.histogram('Response', self.response, family=self.mode)\n",
        "\n",
        "      # Two more metrics to monitor the performance of training\n",
        "      tf.summary.scalar('center_score_error', center_score_error(response), family=self.mode)\n",
        "      tf.summary.scalar('center_dist_error', center_dist_error(response), family=self.mode)\n",
        "\n",
        "  def setup_global_step(self):\n",
        "    global_step = tf.Variable(\n",
        "      initial_value=0,\n",
        "      name='global_step',\n",
        "      trainable=False,\n",
        "      collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])\n",
        "\n",
        "    self.global_step = global_step\n",
        "\n",
        "  def setup_embedding_initializer(self):\n",
        "    \"\"\"Sets up the function to restore embedding variables from checkpoint.\"\"\"\n",
        "    embed_config = self.model_config['embed_config']\n",
        "    if embed_config['embedding_checkpoint_file']:\n",
        "      # Restore Siamese FC models from .mat model files\n",
        "      initialize = load_mat_model(embed_config['embedding_checkpoint_file'],\n",
        "                                  'convolutional_alexnet/', 'detection/')\n",
        "\n",
        "      def restore_fn(sess):\n",
        "        tf.logging.info(\"Restoring embedding variables from checkpoint file %s\",\n",
        "                        embed_config['embedding_checkpoint_file'])\n",
        "        sess.run([initialize])\n",
        "\n",
        "      self.init_fn = restore_fn\n",
        "\n",
        "  def build(self, reuse=False):\n",
        "    \"\"\"Creates all ops for training and evaluation\"\"\"\n",
        "    with tf.name_scope(self.mode):\n",
        "      self.build_inputs()\n",
        "      self.build_image_embeddings(reuse=reuse)\n",
        "      self.build_template()\n",
        "      self.build_detection(reuse=reuse)\n",
        "      self.setup_embedding_initializer()\n",
        "\n",
        "      if self.mode in ['train', 'validation']:\n",
        "        self.build_loss()\n",
        "\n",
        "      if self.is_training():\n",
        "        self.setup_global_step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO3s8_QH79Jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright @ 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "\n",
        "\"\"\"Default configurations of model specification, training and tracking\n",
        "For most of the time, DO NOT modify the configurations within this file.\n",
        "Use the configurations here as the default configurations and only update\n",
        "them following the examples in the `experiments` directory.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "LOG_DIR = 'Logs/SiamFC'  # where checkpoints, logs are saved\n",
        "RUN_NAME = 'SiamFC-3s-color-scratch'  # identifier of the experiment\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "  'z_image_size': 127,  # Exemplar image size\n",
        "\n",
        "  'embed_config': {'embedding_name': 'convolutional_alexnet',\n",
        "                   'embedding_checkpoint_file': None,  # mat file path of the pretrained embedding model.\n",
        "                   'train_embedding': True,\n",
        "                   'init_method': 'kaiming_normal',\n",
        "                   'use_bn': True,\n",
        "                   'bn_scale': True,\n",
        "                   'bn_momentum': 0.05,\n",
        "                   'bn_epsilon': 1e-6,\n",
        "                   'embedding_feature_num': 256,\n",
        "                   'weight_decay': 5e-4,\n",
        "                   'stride': 8, },\n",
        "\n",
        "  'adjust_response_config': {'train_bias': True,\n",
        "                             'scale': 1e-3, },\n",
        "}\n",
        "\n",
        "TRAIN_CONFIG = {\n",
        "  'train_dir': osp.join(LOG_DIR, 'track_model_checkpoints', RUN_NAME),\n",
        "\n",
        "  'seed': 123,  # fix seed for reproducing experiments\n",
        "\n",
        "  'train_data_config': {'input_imdb': 'data/train_imdb.pickle',\n",
        "                        'preprocessing_name': 'siamese_fc_color',\n",
        "                        'num_examples_per_epoch': 5.32e4,\n",
        "                        'epoch': 50,\n",
        "                        'batch_size': 8,\n",
        "                        'max_frame_dist': 100,  # Maximum distance between any two random frames draw from videos.\n",
        "                        'prefetch_threads': 4,\n",
        "                        'prefetch_capacity': 15 * 8, },  # The maximum elements number in the data loading queue\n",
        "\n",
        "  'validation_data_config': {'input_imdb': 'data/validation_imdb.pickle',\n",
        "                             'preprocessing_name': 'None',\n",
        "                             'batch_size': 8,\n",
        "                             'max_frame_dist': 100,  # Maximum distance between any two random frames draw from videos.\n",
        "                             'prefetch_threads': 1,\n",
        "                             'prefetch_capacity': 15 * 8, },  # The maximum elements number in the data loading queue\n",
        "\n",
        "  # Configurations for generating groundtruth maps\n",
        "  'gt_config': {'rPos': 16,\n",
        "                'rNeg': 0, },\n",
        "\n",
        "  # Optimizer for training the model.\n",
        "  'optimizer_config': {'optimizer': 'MOMENTUM',  # SGD and MOMENTUM are supported\n",
        "                       'momentum': 0.9,\n",
        "                       'use_nesterov': False, },\n",
        "\n",
        "  # Learning rate configs\n",
        "  'lr_config': {'policy': 'exponential',\n",
        "                'initial_lr': 0.01,\n",
        "                'num_epochs_per_decay': 1,\n",
        "                'lr_decay_factor': 0.8685113737513527,\n",
        "                'staircase': True, },\n",
        "\n",
        "  # If not None, clip gradients to this value.\n",
        "  'clip_gradients': None,\n",
        "\n",
        "  # Frequency at which loss and global step are logged\n",
        "  'log_every_n_steps': 10,\n",
        "\n",
        "  # Frequency to save model\n",
        "  'save_model_every_n_step': 5.32e4 // 8,  # save model every epoch\n",
        "\n",
        "  # How many model checkpoints to keep. No limit if None.\n",
        "  'max_checkpoints_to_keep': None,\n",
        "}\n",
        "\n",
        "TRACK_CONFIG = {\n",
        "  # Directory for saving log files during tracking.\n",
        "  'log_dir': osp.join(LOG_DIR, 'track_model_inference', RUN_NAME),\n",
        "\n",
        "  # Logging level of inference, use 1 for detailed inspection. 0 for speed.\n",
        "  'log_level': 0,\n",
        "\n",
        "  'x_image_size': 255,  # Search image size during tracking\n",
        "\n",
        "  # Configurations for upsampling score maps\n",
        "  'upsample_method': 'bicubic',\n",
        "  'upsample_factor': 16,\n",
        "\n",
        "  # Configurations for searching scales\n",
        "  'num_scales': 3,  # Number of scales to search\n",
        "  'scale_step': 1.0375,  # Scale changes between different scale search\n",
        "  'scale_damp': 0.59,  # Damping factor for scale update\n",
        "  'scale_penalty': 0.9745,  # Score penalty for scale change\n",
        "\n",
        "  # Configurations for penalizing large displacement from the center\n",
        "  'window_influence': 0.176,\n",
        "\n",
        "  'include_first': False, # If track the first frame\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH20xqmDEH3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright @ 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "\n",
        "\"\"\"Default configurations of model specification, training and tracking\n",
        "For most of the time, DO NOT modify the configurations within this file.\n",
        "Use the configurations here as the default configurations and only update\n",
        "them following the examples in the `experiments` directory.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os.path as osp\n",
        "\n",
        "LOG_DIR = 'Logs/SiamFC'  # where checkpoints, logs are saved\n",
        "RUN_NAME = 'SiamFC-3s-color-scratch'  # identifier of the experiment\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "  'z_image_size': 127,  # Exemplar image size\n",
        "\n",
        "  'embed_config': {'embedding_name': 'convolutional_alexnet',\n",
        "                   'embedding_checkpoint_file': None,  # mat file path of the pretrained embedding model.\n",
        "                   'train_embedding': True,\n",
        "                   'init_method': 'kaiming_normal',\n",
        "                   'use_bn': True,\n",
        "                   'bn_scale': True,\n",
        "                   'bn_momentum': 0.05,\n",
        "                   'bn_epsilon': 1e-6,\n",
        "                   'embedding_feature_num': 256,\n",
        "                   'weight_decay': 5e-4,\n",
        "                   'stride': 8, },\n",
        "\n",
        "  'adjust_response_config': {'train_bias': True,\n",
        "                             'scale': 1e-3, },\n",
        "}\n",
        "\n",
        "TRAIN_CONFIG = {\n",
        "  'train_dir': osp.join(LOG_DIR, 'track_model_checkpoints', RUN_NAME),\n",
        "\n",
        "  'seed': 123,  # fix seed for reproducing experiments\n",
        "\n",
        "  'train_data_config': {'input_imdb': 'data/train_imdb.pickle',\n",
        "                        'preprocessing_name': 'siamese_fc_color',\n",
        "                        'num_examples_per_epoch': 5.32e4,\n",
        "                        'epoch': 50,\n",
        "                        'batch_size': 8,\n",
        "                        'max_frame_dist': 100,  # Maximum distance between any two random frames draw from videos.\n",
        "                        'prefetch_threads': 4,\n",
        "                        'prefetch_capacity': 15 * 8, },  # The maximum elements number in the data loading queue\n",
        "\n",
        "  'validation_data_config': {'input_imdb': 'data/validation_imdb.pickle',\n",
        "                             'preprocessing_name': 'None',\n",
        "                             'batch_size': 8,\n",
        "                             'max_frame_dist': 100,  # Maximum distance between any two random frames draw from videos.\n",
        "                             'prefetch_threads': 1,\n",
        "                             'prefetch_capacity': 15 * 8, },  # The maximum elements number in the data loading queue\n",
        "\n",
        "  # Configurations for generating groundtruth maps\n",
        "  'gt_config': {'rPos': 16,\n",
        "                'rNeg': 0, },\n",
        "\n",
        "  # Optimizer for training the model.\n",
        "  'optimizer_config': {'optimizer': 'MOMENTUM',  # SGD and MOMENTUM are supported\n",
        "                       'momentum': 0.9,\n",
        "                       'use_nesterov': False, },\n",
        "\n",
        "  # Learning rate configs\n",
        "  'lr_config': {'policy': 'exponential',\n",
        "                'initial_lr': 0.01,\n",
        "                'num_epochs_per_decay': 1,\n",
        "                'lr_decay_factor': 0.8685113737513527,\n",
        "                'staircase': True, },\n",
        "\n",
        "  # If not None, clip gradients to this value.\n",
        "  'clip_gradients': None,\n",
        "\n",
        "  # Frequency at which loss and global step are logged\n",
        "  'log_every_n_steps': 10,\n",
        "\n",
        "  # Frequency to save model\n",
        "  'save_model_every_n_step': 5.32e4 // 8,  # save model every epoch\n",
        "\n",
        "  # How many model checkpoints to keep. No limit if None.\n",
        "  'max_checkpoints_to_keep': None,\n",
        "}\n",
        "\n",
        "TRACK_CONFIG = {\n",
        "  # Directory for saving log files during tracking.\n",
        "  'log_dir': osp.join(LOG_DIR, 'track_model_inference', RUN_NAME),\n",
        "\n",
        "  # Logging level of inference, use 1 for detailed inspection. 0 for speed.\n",
        "  'log_level': 0,\n",
        "\n",
        "  'x_image_size': 255,  # Search image size during tracking\n",
        "\n",
        "  # Configurations for upsampling score maps\n",
        "  'upsample_method': 'bicubic',\n",
        "  'upsample_factor': 16,\n",
        "\n",
        "  # Configurations for searching scales\n",
        "  'num_scales': 3,  # Number of scales to search\n",
        "  'scale_step': 1.0375,  # Scale changes between different scale search\n",
        "  'scale_damp': 0.59,  # Damping factor for scale update\n",
        "  'scale_penalty': 0.9745,  # Score penalty for scale change\n",
        "\n",
        "  # Configurations for penalizing large displacement from the center\n",
        "  'window_influence': 0.176,\n",
        "\n",
        "  'include_first': False, # If track the first frame\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBqIEzND_HNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "5e3229a1-fddc-4d65-b881-7877573103b6"
      },
      "source": [
        "#! /usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "#\n",
        "# Copyright Â© 2017 bily     Huazhong University of Science and Technology\n",
        "#\n",
        "# Distributed under terms of the MIT license.\n",
        "\n",
        "\"\"\"Train the model\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import os.path as osp\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sacred import Experiment\n",
        "from sacred.observers import FileStorageObserver\n",
        "\n",
        "ex = Experiment(configuration.RUN_NAME)\n",
        "ex.observers.append(FileStorageObserver.create(osp.join(configuration.LOG_DIR, 'sacred')))\n",
        "\n",
        "\n",
        "\n",
        "@ex.config\n",
        "def configurations():\n",
        "  # Add configurations for current script, for more details please see the documentation of `sacred`.\n",
        "  # REFER: http://sacred.readthedocs.io/en/latest/index.html\n",
        "  model_config = configuration.MODEL_CONFIG\n",
        "  train_config = configuration.TRAIN_CONFIG\n",
        "  track_config = configuration.TRACK_CONFIG\n",
        "\n",
        "\n",
        "def _configure_learning_rate(train_config, global_step):\n",
        "  lr_config = train_config['lr_config']\n",
        "\n",
        "  num_batches_per_epoch = \\\n",
        "    int(train_config['train_data_config']['num_examples_per_epoch'] / train_config['train_data_config']['batch_size'])\n",
        "\n",
        "  lr_policy = lr_config['policy']\n",
        "  if lr_policy == 'piecewise_constant':\n",
        "    lr_boundaries = [int(e * num_batches_per_epoch) for e in lr_config['lr_boundaries']]\n",
        "    return tf.train.piecewise_constant(global_step,\n",
        "                                       lr_boundaries,\n",
        "                                       lr_config['lr_values'])\n",
        "  elif lr_policy == 'exponential':\n",
        "    decay_steps = int(num_batches_per_epoch) * lr_config['num_epochs_per_decay']\n",
        "    return tf.train.exponential_decay(lr_config['initial_lr'],\n",
        "                                      global_step,\n",
        "                                      decay_steps=decay_steps,\n",
        "                                      decay_rate=lr_config['lr_decay_factor'],\n",
        "                                      staircase=lr_config['staircase'])\n",
        "  elif lr_policy == 'cosine':\n",
        "    T_total = train_config['train_data_config']['epoch'] * num_batches_per_epoch\n",
        "    return 0.5 * lr_config['initial_lr'] * (1 + tf.cos(np.pi * tf.to_float(global_step) / T_total))\n",
        "  else:\n",
        "    raise ValueError('Learning rate policy [%s] was not recognized', lr_policy)\n",
        "\n",
        "\n",
        "def _configure_optimizer(train_config, learning_rate):\n",
        "  optimizer_config = train_config['optimizer_config']\n",
        "  optimizer_name = optimizer_config['optimizer'].upper()\n",
        "  if optimizer_name == 'MOMENTUM':\n",
        "    optimizer = tf.train.MomentumOptimizer(\n",
        "      learning_rate,\n",
        "      momentum=optimizer_config['momentum'],\n",
        "      use_nesterov=optimizer_config['use_nesterov'],\n",
        "      name='Momentum')\n",
        "  elif optimizer_name == 'SGD':\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  else:\n",
        "    raise ValueError('Optimizer [%s] was not recognized', optimizer_config['optimizer'])\n",
        "  return optimizer\n",
        "\n",
        "\n",
        "@ex.automain\n",
        "def main(model_config, train_config, track_config):\n",
        "  os.environ['CUDA_VISIBLE_DEVICES'] = auto_select_gpu()\n",
        "\n",
        "  # Create training directory which will be used to save: configurations, model files, TensorBoard logs\n",
        "  train_dir = train_config['train_dir']\n",
        "  if not osp.isdir(train_dir):\n",
        "    logging.info('Creating training directory: %s', train_dir)\n",
        "    mkdir_p(train_dir)\n",
        "\n",
        "  g = tf.Graph()\n",
        "  with g.as_default():\n",
        "    # Set fixed seed for reproducible experiments\n",
        "    random.seed(train_config['seed'])\n",
        "    np.random.seed(train_config['seed'])\n",
        "    tf.set_random_seed(train_config['seed'])\n",
        "\n",
        "    # Build the training and validation model\n",
        "    model = siamese_model.SiameseModel(model_config, train_config, mode='train')\n",
        "    model.build()\n",
        "    model_va = siamese_model.SiameseModel(model_config, train_config, mode='validation')\n",
        "    model_va.build(reuse=True)\n",
        "\n",
        "    # Save configurations for future reference\n",
        "    save_cfgs(train_dir, model_config, train_config, track_config)\n",
        "\n",
        "    learning_rate = _configure_learning_rate(train_config, model.global_step)\n",
        "    optimizer = _configure_optimizer(train_config, learning_rate)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "\n",
        "    # Set up the training ops\n",
        "    opt_op = tf.contrib.layers.optimize_loss(\n",
        "      loss=model.total_loss,\n",
        "      global_step=model.global_step,\n",
        "      learning_rate=learning_rate,\n",
        "      optimizer=optimizer,\n",
        "      clip_gradients=train_config['clip_gradients'],\n",
        "      learning_rate_decay_fn=None,\n",
        "      summaries=['learning_rate'])\n",
        "\n",
        "    with tf.control_dependencies([opt_op]):\n",
        "      train_op = tf.no_op(name='train')\n",
        "\n",
        "    saver = tf.train.Saver(tf.global_variables(),\n",
        "                           max_to_keep=train_config['max_checkpoints_to_keep'])\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter(train_dir, g)\n",
        "    summary_op = tf.summary.merge_all()\n",
        "\n",
        "    global_variables_init_op = tf.global_variables_initializer()\n",
        "    local_variables_init_op = tf.local_variables_initializer()\n",
        "    g.finalize()  # Finalize graph to avoid adding ops by mistake\n",
        "\n",
        "    # Dynamically allocate GPU memory\n",
        "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "    sess_config = tf.ConfigProto(gpu_options=gpu_options)\n",
        "\n",
        "    sess = tf.Session(config=sess_config)\n",
        "    model_path = tf.train.latest_checkpoint(train_config['train_dir'])\n",
        "\n",
        "    if not model_path:\n",
        "      sess.run(global_variables_init_op)\n",
        "      sess.run(local_variables_init_op)\n",
        "      start_step = 0\n",
        "\n",
        "      if model_config['embed_config']['embedding_checkpoint_file']:\n",
        "        model.init_fn(sess)\n",
        "    else:\n",
        "      logging.info('Restore from last checkpoint: {}'.format(model_path))\n",
        "      sess.run(local_variables_init_op)\n",
        "      saver.restore(sess, model_path)\n",
        "      start_step = tf.train.global_step(sess, model.global_step.name) + 1\n",
        "\n",
        "    # Training loop\n",
        "    data_config = train_config['train_data_config']\n",
        "    total_steps = int(data_config['epoch'] *\n",
        "                      data_config['num_examples_per_epoch'] /\n",
        "                      data_config['batch_size'])\n",
        "    logging.info('Train for {} steps'.format(total_steps))\n",
        "    for step in range(start_step, total_steps):\n",
        "      start_time = time.time()\n",
        "      _, loss, batch_loss = sess.run([train_op, model.total_loss, model.batch_loss])\n",
        "      duration = time.time() - start_time\n",
        "\n",
        "      if step % 10 == 0:\n",
        "        examples_per_sec = data_config['batch_size'] / float(duration)\n",
        "        time_remain = data_config['batch_size'] * (total_steps - step) / examples_per_sec\n",
        "        m, s = divmod(time_remain, 60)\n",
        "        h, m = divmod(m, 60)\n",
        "        format_str = ('%s: step %d, total loss = %.2f, batch loss = %.2f (%.1f examples/sec; %.3f '\n",
        "                      'sec/batch; %dh:%02dm:%02ds remains)')\n",
        "        logging.info(format_str % (datetime.now(), step, loss, batch_loss,\n",
        "                                   examples_per_sec, duration, h, m, s))\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        summary_str = sess.run(summary_op)\n",
        "        summary_writer.add_summary(summary_str, step)\n",
        "\n",
        "      if step % train_config['save_model_every_n_step'] == 0 or (step + 1) == total_steps:\n",
        "        checkpoint_path = osp.join(train_config['train_dir'], 'model.ckpt')\n",
        "        saver.save(sess, checkpoint_path, global_step=step)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6acb014bf46e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msacred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileStorageObserver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUN_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileStorageObserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sacred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'configuration' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeFXaamuC6KC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "0e9aac41-57e9-4a3c-9336-de934be4819a"
      },
      "source": [
        "!pip install sacred\n",
        "\n",
        "# ç¨äºæ°æ®åºè¿æ¥\n",
        "!pip install numpy pymongo\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacred\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/7f/c5679977f1eceac432c59cc92bd1ddb7272c282c3db8eb846d0e1c03b6a0/sacred-0.8.1.tar.gz (90kB)\n",
            "\r\u001b[K     |ââââ                            | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |ââââââââ                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |âââââââââââ                     | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââ                 | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââ              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââ          | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââ      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |âââââââââââââââââââââââââââââ   | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |ââââââââââââââââââââââââââââââââ| 92kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt<1.0,>=0.3 in /usr/local/lib/python3.6/dist-packages (from sacred) (0.6.2)\n",
            "Collecting jsonpickle<2.0,>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/e0/54421447d55bc7304a785be9ec81f28e1e8a8c6619b0e35154ed8f1b7761/jsonpickle-1.4-py2.py3-none-any.whl\n",
            "Collecting munch<3.0,>=2.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from sacred) (1.12.1)\n",
            "Collecting py-cpuinfo>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/60/63f28a5401da733043abe7053e7d9591491b4784c4f87c339bf51215aa0a/py-cpuinfo-5.0.0.tar.gz (82kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 92kB 6.7MB/s \n",
            "\u001b[?25hCollecting colorama>=0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.6/dist-packages (from sacred) (20.3)\n",
            "Collecting GitPython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/1a/0df85d2bddbca33665d2148173d3281b290ac054b5f50163ea735740ac7b/GitPython-3.1.1-py3-none-any.whl (450kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 460kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle<2.0,>=1.2->sacred) (1.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch<3.0,>=2.0.2->sacred) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=18.0->sacred) (2.4.7)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/52/ca35448b56c53a079d3ffe18b1978c6e424f6d4df02404877094c89f5bfb/gitdb-4.0.4-py3-none-any.whl (63kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 71kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle<2.0,>=1.2->sacred) (3.1.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: sacred, py-cpuinfo\n",
            "  Building wheel for sacred (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacred: filename=sacred-0.8.1-py2.py3-none-any.whl size=105018 sha256=2f78c925ed856415fab1433c9b4a37964cd9127e9cfd1fef49d5f8b396f1017b\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/a8/f6/1d5f073245cb0a221962713adf81e56c1c9608083f85ecac9b\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-5.0.0-cp36-none-any.whl size=18684 sha256=1293cc0e17cbc5234ac43bc7c9d6af27ba57e32e703acc92b724ef9e402ed45f\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/7e/a9/b982d0fea22b7e4ae5619de949570cde5ad55420cec16e86a5\n",
            "Successfully built sacred py-cpuinfo\n",
            "Installing collected packages: jsonpickle, munch, py-cpuinfo, colorama, smmap, gitdb, GitPython, sacred\n",
            "Successfully installed GitPython-3.1.1 colorama-0.4.3 gitdb-4.0.4 jsonpickle-1.4 munch-2.5.0 py-cpuinfo-5.0.0 sacred-0.8.1 smmap-3.0.2\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (3.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}